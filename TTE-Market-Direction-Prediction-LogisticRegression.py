# ────────────────────────────────────────────────────────────────  
# 1. IMPORTING LIBRARIES  
# ────────────────────────────────────────────────────────────────  

# Visualization
import matplotlib.pyplot as plt   # Basic visualization tools (line plot, bar plot...)
import seaborn as sn              # For heatmaps

# Financial data
import yfinance as yf             # Market data retrieval (Yahoo Finance)

# Numerical operations
import numpy as np                # Numerical calculations (rolling, log, etc.)
import pandas as pd               # DataFrame manipulations (filtering, cleaning, concatenation...)

# Machine Learning
from sklearn.linear_model import LogisticRegression  # Binary classification model
from sklearn.model_selection import TimeSeriesSplit  # Time-based split to preserve data order
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import roc_auc_score, roc_curve  # Performance metrics for classification

pd.set_option("display.max_columns", None)  # Display all columns of a DataFrame

# ────────────────────────────────────────────────────────────────  
# 2. DOWNLOAD DATA & COMPUTE LOG RETURNS  
# ────────────────────────────────────────────────────────────────  

# Download TotalEnergies stock data (ticker: "TTE") since 2000
data = yf.download("TTE", start="2000-01-01", end="2025-07-16")

# Compute daily logarithmic return (more appropriate for statistical analysis)
data["Log_Return"] = np.log(data["Close"] / data["Close"].shift(1))

# ────────────────────────────────────────────────────────────────  
# 3. ADD CLASSIC TECHNICAL INDICATORS  
# ────────────────────────────────────────────────────────────────  

# Moving averages (short and long periods) on log returns
data["MA10"] = data["Log_Return"].rolling(10).mean()
data["MA50"] = data["Log_Return"].rolling(50).mean()

# Detecting moving average crossovers (bullish signal)
data["MA Crossover"] = np.where(data["MA10"] > data["MA50"], 1, 0)

# Rolling standard deviations (historical volatility)
data["MSTD10"] = data["Log_Return"].rolling(10).std()
data["MSTD50"] = data["Log_Return"].rolling(50).std()

# RSI (Relative Strength Index) calculation over 14 days
avg_profit = data["Log_Return"].clip(lower=0).rolling(14).mean()
avg_loss = data["Log_Return"].clip(upper=0).rolling(14).mean()
RS = avg_profit / (-avg_loss)
RSI = 100 - (100 / (1 + RS))
data["RSI"] = RSI

# Rolling Sharpe Ratio (14 days)
risk_free_rate = 0.0202  # Annualized risk-free rate (e.g., 10-year OAT)
trading_days = 252       # Number of trading days per year
daily_risk_free_rate = risk_free_rate / trading_days

sharpe_ratio = (data["Log_Return"].rolling(14).mean() - daily_risk_free_rate) / data["Log_Return"].rolling(14).std()
data["Sharpe Ratio"] = sharpe_ratio

# ────────────────────────────────────────────────────────────────  
# 3. Machine Learning Phase (binary classification)
# ────────────────────────────────────────────────────────────────  

# Create the target variable ("Target"):
# # 1 if the price increases by more than 0.5% compared to the previous day
data["Target"] = np.where(data["Close"].pct_change()>0.005, 1, 0)

# Data preparation
data.dropna(inplace=True)  # Remove rows with NaNs generated by indicators

# Split features / target
X = data.drop("Target", axis=1)
y = data["Target"]

# Time-based split: preserve chronological order (testing on 70%, training on remaining 30%)
# In quantitative finance, it's crucial to avoid future data leakage into training

dates = X.index   # Keep dates for plotting purposes
X_index_reset = X.reset_index(drop=True)
y_index_reset = y.reset_index(drop=True)

tscv = TimeSeriesSplit(n_splits=5, max_train_size=int(0.7 * len(X)))  # Split with 70% max for training

for train_index, test_index in tscv.split(X_index_reset):
    X_train, X_test = X_index_reset.iloc[train_index], X_index_reset.iloc[test_index]
    y_train, y_test = y_index_reset.iloc[train_index], y_index_reset.iloc[test_index]

# Logistic regression
lr = LogisticRegression(random_state=42)
lr.fit(X_train, y_train)

# Predictions
y_pred_proba = lr.predict_proba(X_test)  # Probability of class 1
y_pred = lr.predict(X_test)              # Predicted class

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1])

# Result reporting
print("=========================================================================== ")
print("RESULTS - Training with ALL features (raw, unfiltered)")
print("=========================================================================== ")
print("---------------------------------------------------------------------------")
print("-------------------Here are predictions and probabilities ----------------")
print(pd.DataFrame({
    "Predictions": y_pred[0:10].flatten(),
    "Proba 0": y_pred_proba[:10, 0],
    "Proba 1": y_pred_proba[:10, 1]
}))
print("---------------------------------------------------------------------------")
print("-----------------------ROC/AUC Score--------------------------")
print(roc_auc_score(y_test, y_pred_proba[:, 1]))
print("-----------------------Confusion matrix------------------------------")
print(confusion_matrix(y_test, y_pred))
print("-----------------------Classification report--------------------------")
print(classification_report(y_test, y_pred))

# Retrain with filtered features (remove low-significance features due to weak coefficients)

data.dropna(inplace=True)
X = data.drop(["Target", "Volume", "MA50", "MSTD10"], axis=1)  # Drop irrelevant variables
y = data["Target"]

# Same time series split
dates = X.index
X_index_reset = X.reset_index(drop=True)
y_index_reset = y.reset_index(drop=True)

tscv = TimeSeriesSplit(n_splits=5, max_train_size=int(0.7 * len(X)))
for train_index, test_index in tscv.split(X_index_reset):
    X_train, X_test = X_index_reset.iloc[train_index], X_index_reset.iloc[test_index]
    y_train, y_test = y_index_reset.iloc[train_index], y_index_reset.iloc[test_index]

# Logistic regression on new selected features
lr = LogisticRegression(random_state=42)
lr.fit(X_train, y_train)

y_pred_proba = lr.predict_proba(X_test)
y_pred = lr.predict(X_test)

fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1])

# Result reporting
print("=========================================================================== ")
print("RESULTS - Training with **filtered** features (after cleaning)")
print("=========================================================================== ")

print("---------------------------------------------------------------------------")
print("-------------------Here are predictions and probabilities ----------------")
print(pd.DataFrame({
    "Predictions": y_pred[0:10].flatten(),
    "Proba 0": y_pred_proba[:10, 0],
    "Proba 1": y_pred_proba[:10, 1]
}))
print("---------------------------------------------------------------------------")
print("-----------------------ROC/AUC Score--------------------------")
print(roc_auc_score(y_test, y_pred_proba[:, 1]))
print("-----------------------Confusion matrix------------------------------")
print(confusion_matrix(y_test, y_pred))
print("-----------------------Classification report--------------------------")
print(classification_report(y_test, y_pred))

# Trading strategy based on filtered predictions

df_strategy = pd.DataFrame({
    "Predictions": y_pred,
    "Proba (1)": y_pred_proba[:, 1],
    "Open": data["Open"].iloc[test_index].values.flatten(),
    "Close": data["Close"].iloc[test_index].values.flatten()
}, index=data.index[test_index])

def run_strategy_with_grid_search(df_strategy, proba_range=np.arange(0.5, 0.91, 0.05), 
                                  stoploss_range=np.arange(-0.05, -0.009, 0.005)):
    results = []
    for prob in proba_range:
        for stop in stoploss_range:
            df_strategy["Signal"] = np.where((df_strategy["Predictions"] == 1) & (df_strategy["Proba (1)"] >= prob), 1, 0)    # Signal: Only trade if predicted to increase AND the model is confident enough
            df_strategy["Position"] = df_strategy["Signal"].shift(1)    # Enter the trade the next day
            df_strategy["Return"] = df_strategy["Close"] / df_strategy["Open"] - 1    # Compute daily return from Open to Close
            df_strategy["StopLoss"] = np.where(df_strategy["Return"] + df_strategy["Return"].shift(1) < stop, 1, 0)   # Stop loss condition: if today's + yesterday's return drops below stop threshold, exit
            df_strategy["Strategy"] = np.where(df_strategy["StopLoss"] == 0, df_strategy["Position"] * df_strategy["Return"], 0)   # Strategy return: only apply return if no stop loss triggered
            df_strategy["Cumulative_return"] = (1 + df_strategy["Strategy"]).cumprod()   # Cumulative return of the strategy over time
            # Compute performance metrics
            performance = ((df_strategy.iloc[-1,-1]-1)/1)* 100
            highest_performance = df_strategy["Cumulative_return"].max()
            lowest_performance = df_strategy["Cumulative_return"].min()
            perf_max = ((highest_performance-1)/1)*100
            perf_min = ((lowest_performance-1)/1)*100
            highest_return = df_strategy["Strategy"].max()
            lowest_return = df_strategy["Strategy"].min()
            # Append all metrics for this parameter combination
            results.append({
                "Prob_value": prob,
                "StopLoss_value": stop,
                "Performance": performance,
                "Highest_Performance": highest_performance,
                "Lowest_Performance": lowest_performance,
                "Perf_Max": perf_max,
                "Perf_Min": perf_min,
                "Highest_Return": highest_return,
                "Lowest_Return": lowest_return
            })
    # Return the results sorted by overall performance (best first)
    results_df = pd.DataFrame(results)
    results_df.sort_values(by="Performance", ascending=False, inplace=True)
    return results_df

# Run grid search to find optimal probability threshold and stop-loss level
results_df = run_strategy_with_grid_search(df_strategy)
best_performances = results_df.head(3)  # Show top 3 parameter sets
print("Best performances from grid search:")
print(best_performances)

# Extract best parameters (first row in sorted DataFrame)
best_params = {"Prob_value": round(float(results_df.iloc[0]["Prob_value"]), 2),
               "Stop_loss_value": round(float(results_df.iloc[0]["StopLoss_value"]), 3)}

# Recompute strategy using optimal parameters found via grid search
best_prob = best_params["Prob_value"]
best_stop = best_params["Stop_loss_value"]

df_strategy["Signal"] = np.where((df_strategy["Predictions"] == 1) & (df_strategy["Proba (1)"] >= best_prob), 1, 0)
df_strategy["Position"] = df_strategy["Signal"].shift(1)
df_strategy["Return"] = df_strategy["Close"] / df_strategy["Open"] - 1
df_strategy["StopLoss"] = np.where(df_strategy["Return"] + df_strategy["Return"].shift(1) < best_stop, 1, 0)
df_strategy["Strategy"] = np.where(df_strategy["StopLoss"] == 0, df_strategy["Position"] * df_strategy["Return"], 0)
df_strategy["Cumulative_return"] = (1 + df_strategy["Strategy"]).cumprod()

# Print final performance metrics of the optimized strategy
print("Final performance of the optimized strategy:")
print("Performance: {:.2f}%".format(((df_strategy["Cumulative_return"].iloc[-1] - 1) / 1) * 100))
print("Best parameters found :", best_params)
print("Initial Capital: {:.2f}".format(df_strategy["Cumulative_return"].iloc[1]))
print("Final Capital: {:.2f}".format(df_strategy["Cumulative_return"].iloc[-1]))
print("Performance Mean: {:.2f}%".format(((df_strategy["Cumulative_return"].mean() - 1) / 1) * 100))
print("Performance Standard Deviation: {:.2f}%".format(((df_strategy["Cumulative_return"].std() - 1) / 1) * 100))
print("Performance Max: {:.2f}%".format(((df_strategy["Cumulative_return"].max() - 1) / 1) * 100))
print("Performance Min: {:.2f}%".format(((df_strategy["Cumulative_return"].min() - 1) / 1) * 100))
print("Max Return: {:.2f}%".format(((df_strategy["Strategy"].max() - 1) / 1) * 100))
print("Min Return: {:.2f}%".format(((df_strategy["Strategy"].min() - 1) / 1) * 100))

# ────────────────────────────────────────────────────────────────  
# VISUALIZATIONS
# ────────────────────────────────────────────────────────────────  

# ROC Curve - Raw Features
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot([0, 1], [0, 1], color="skyblue", linestyle='--', label='Random baseline')
plt.plot(fpr, tpr, color="green", label="Logistic Model")
plt.title("ROC Curve (Raw Features) - Model Performance (ROC/AUC Score={})".format(round(roc_auc_score(y_test, y_pred_proba[:, 1]),4)))
plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (TPR)")
plt.legend(loc='lower right')

# Confusion Matrix - Raw Features
plt.subplot(1, 2, 2)
sn.heatmap(confusion_matrix(y_test, y_pred), cmap="Blues", annot=True, fmt="d")
plt.title("Confusion Matrix - Raw Features")
plt.xlabel("Predictions")
plt.ylabel("True Values")
plt.tight_layout()
plt.show()

# ROC Curve - Filtered Features
plt.subplot(1, 2, 1)
plt.plot([0, 1], [0, 1], color="skyblue", linestyle='--', label='Random baseline')
plt.plot(fpr, tpr, color="green", label="Logistic Model")
plt.title("ROC Curve (Filtered Features) - Model Performance (ROC/AUC Score={})".format(round(roc_auc_score(y_test, y_pred_proba[:, 1]),4)))
plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (TPR)")
plt.legend(loc='lower right')

# Confusion Matrix - Filtered Features
plt.subplot(1, 2, 2)
sn.heatmap(confusion_matrix(y_test, y_pred), cmap="Blues", annot=True, fmt="d")
plt.title("Confusion Matrix - Filtered Features")
plt.xlabel("Predictions")
plt.ylabel("True Values")
plt.tight_layout()
plt.show()

# Feature Importance - Raw Features
plt.figure(figsize=(10, 5))
plt.subplot(2,1,1)
plt.bar([str(col) for col in X.columns], lr.coef_.flatten(), color='orange')
plt.xticks(rotation=45, ha="right")
plt.title("Feature Importance - Logistic Regression - Raw Features")
plt.xlabel("Features")
plt.ylabel("Coefficient Weight")
plt.tight_layout()
plt.show()

# Feature Importance - Filtered Features
plt.figure(figsize=(10, 5))
plt.subplot(2,1,2)
plt.bar([str(col) for col in X.columns], lr.coef_.flatten(), color='orange')
plt.xticks(rotation=45, ha="right")
plt.title("Feature Importance - Logistic Regression - Filtered Features")
plt.xlabel("Features")
plt.ylabel("Coefficient Weight")
plt.tight_layout()
plt.show()

# Feature Correlation Matrix
features_corr = X.corr()
plt.figure(figsize=(12, 6))
sn.heatmap(features_corr, cmap="coolwarm", annot=True, fmt=".2f", square=True, cbar_kws={"shrink": 0.75})
plt.title("Matrix of Correlations Between Features", fontsize=14)
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()

# Cumulative Return - Optimized Trading Strategy
plt.figure(figsize=(10, 5))
plt.plot(df_strategy["Cumulative_return"], label="Prob>={}, Stop_loss<={}".format(round(best_prob, 2), round(best_stop, 3)), color="blue")
plt.title("Optimized Trading Strategy with Best Parameters")
plt.xlabel("Date")
plt.ylabel("Cumulative Return")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
